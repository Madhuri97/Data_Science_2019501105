{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error as mserr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read train and test datasets into pandas DataFrames trainx_df, trainy_df,testx_df\n",
    "def readDataSets(train_path, test_path, predict_col, index_col = None):\n",
    "    if index_col == None:\n",
    "        trainx_df = pd.read_csv(train_path)\n",
    "        trainy_df = trainx_df[predict_col]\n",
    "        trainy_df.hist()\n",
    "        trainx_df.drop(predict_col,axis = 1,inplace = True)\n",
    "        testx_df = pd.read_csv(test_path)\n",
    "    else:\n",
    "        trainx_df = pd.read_csv(train_path,index_col = 'Id')\n",
    "        trainy_df = trainx_df[predict_col]\n",
    "        trainx_df.drop(predict_col,axis = 1,inplace = True)\n",
    "        testx_df = pd.read_csv(test_path,index_col='Id')\n",
    "    return trainx_df,trainy_df,testx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a first step of pre-processing remove columns with null value ratio greater than provided limit\n",
    "def dropFeturesWithNullValuesGreaterThanALimit(trainx_df, testx_df, null_ratio = 0.3):\n",
    "    sample_size = len(trainx_df)\n",
    "    columns_with_null_values = [[col, float(trainx_df[col].isnull().sum())/float(sample_size)] for col in trainx_df.columns if trainx_df[col].isnull().sum()]\n",
    "    columns_to_drop = [x for (x,y) in columns_with_null_values if y > null_ratio]\n",
    "    trainx_df.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "    testx_df.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "    return trainx_df,testx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a second pre-processing step find all categorical columns and one hot  encode them. Before one hot encode fill all null values with dummy in those columns.  Some categorical columns in trainx_df may not have null values in trainx_df but have null values in testx_df. To overcome this problem we will add a row to the trainx_df with all dummy values for categorical values. Once one hot encoding is complete drop the added dummy column\n",
    "def oneHotEncode(trainx_df,testx_df):\n",
    "    categorical_columns = [col for col in trainx_df.columns if trainx_df[col].dtype == object]\n",
    "    ordinal_columns = [col for col in trainx_df.columns if col not in categorical_columns]\n",
    "    dummy_row = list()\n",
    "    for col in trainx_df.columns:\n",
    "        if col in categorical_columns:\n",
    "            dummy_row.append(\"dummy\")\n",
    "        else:\n",
    "            dummy_row.append(\"\")\n",
    "    new_row = pd.DataFrame([dummy_row], columns = trainx_df.columns)\n",
    "    trainx_df = pd.concat([trainx_df,new_row], axis = 0, ignore_index = True)\n",
    "    testx_df = pd.concat([testx_df],axis = 0, ignore_index = True)\n",
    "    for col in categorical_columns:\n",
    "        trainx_df[col].fillna(value = \"dummy\", inplace = True)\n",
    "        testx_df[col].fillna(value = \"dummy\", inplace = True)\n",
    "    enc = OneHotEncoder(drop = 'first', sparse = False)\n",
    "    enc.fit(trainx_df[categorical_columns])\n",
    "    trainx_enc = pd.DataFrame(enc.transform(trainx_df[categorical_columns]))\n",
    "    testx_enc = pd.DataFrame(enc.transform(testx_df[categorical_columns]))\n",
    "    trainx_enc.columns = enc.get_feature_names(categorical_columns)\n",
    "    testx_enc.columns = enc.get_feature_names(categorical_columns)\n",
    "    trainx_df = pd.concat([trainx_df[ordinal_columns], trainx_enc], axis = 1, ignore_index = True)\n",
    "    testx_df = pd.concat([testx_df[ordinal_columns], testx_enc], axis = 1, ignore_index = True)\n",
    "    trainx_df.drop(trainx_df.tail(1).index, inplace = True)\n",
    "    return trainx_df,testx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a third step of pre-processing fill all missing values for ordinal features\n",
    "def fillMissingValues(trainx_df,testx_df):\n",
    "    imputer = KNNImputer(n_neighbors = 2)\n",
    "    imputer.fit(trainx_df)\n",
    "    trainx_df_filled = imputer.transform(trainx_df)\n",
    "    trainx_df_filled = pd.DataFrame(trainx_df_filled, columns = trainx_df.columns)\n",
    "    testx_df_filled = imputer.transform(testx_df)\n",
    "    testx_df_filled = pd.DataFrame(testx_df_filled, columns = testx_df.columns)\n",
    "    testx_df_filled.reset_index(drop = True, inplace = True)\n",
    "    return trainx_df_filled,testx_df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a fourth step of pre-processing scale all the features either through Standard scores or MinMax scaling\n",
    "def scaleFetures(trainx_df,testx_df,scale = 'Standard'):\n",
    "    if scale == 'Standard':\n",
    "        scaler = preprocessing.StandardScaler().fit(trainx_df)\n",
    "        trainx_df = scaler.transform(trainx_df)\n",
    "        testx_df = scaler.transform(testx_df)\n",
    "    elif scale == 'MinMax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(trainx_df)\n",
    "        trainx_df = scaler.transform(trainx_df)\n",
    "        testx_df = scaler.transform(testx_df)\n",
    "    return trainx_df,testx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As fifth step of preprocessing apply PCA\n",
    "def findPrincipalComponents(trainx_df, testx_df):\n",
    "    pca = PCA().fit(trainx_df)\n",
    "    itemindex = np.where(np.cumsum(pca.explained_variance_ratio_) > 0.9999)\n",
    "    print('np.cumsum(pca.explained_variance_ratio_)', np.cumsum(pca.explained_variance_ratio_))\n",
    "    #Plotting the Cumulative Summation of the Explained Variance\n",
    "    plt.figure(np.cumsum(pca.explained_variance_ratio_)[0])\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Variance (%)') \n",
    "    #for each component\n",
    "    plt.title('Principal Components Explained Variance')\n",
    "    plt.show()\n",
    "    pca_std = PCA(n_components=itemindex[0][0]).fit(trainx_df)\n",
    "    trainx_df = pca_std.transform(trainx_df)\n",
    "    testx_df = pca_std.transform(testx_df)\n",
    "    return trainx_df, testx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change labels into 0 and 1\n",
    "def encodeLabelsToZeroAndOne(trainy_df):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    trainy_df = le.fit_transform(trainy_df)\n",
    "    return trainy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a fifth step of pre-processing split the trainx_df into tow parts to build a model and test how is it working to pick best model\n",
    "def splitTrainAndTest(trainx_df, trainy_df, split_ratio = 0.3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(trainx_df, trainy_df.values.ravel(), test_size = split_ratio, random_state = 42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Logistic Regression Model\n",
    "def getLogisticRegressionModel(X_train, y_train, reg_par = 0.00001, max_iterations = 1000000):\n",
    "    logisticreg = LogisticRegression(class_weight = \"balanced\", C = reg_par, max_iter = max_iterations)\n",
    "    logisticreg.fit(X_train, y_train)\n",
    "    return logisticreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit SVM Classification Model\n",
    "def getSVClassificationModel(X_train, y_train, reg_par = 1.0, deg = 3, ker = 'rbf'):\n",
    "    svcmodel = SVC(C = reg_par, degree = deg, kernel=ker)\n",
    "    svcmodel.fit(X_train, y_train)\n",
    "    return svcmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get BackPropagation Model\n",
    "def getBackPropagationModel(X_train, y_train,sol = 'lbfgs', reg_par = 1e-5, hid_layer_sizes = (5, ), random_state = 1, maxi_iter = 1000):\n",
    "    nn_bp_model = MLPClassifier(solver = sol, alpha = reg_par, hidden_layer_sizes = hid_layer_sizes, random_state = 1, max_iter = maxi_iter)\n",
    "    nn_bp_model.fit(X_train, y_train)\n",
    "    return nn_bp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get results from model\n",
    "def getScores(model,X_train, X_test, y_train, y_test):\n",
    "    '''THRESHOLD = 0.5\n",
    "    yhat = np.where(model.predict_proba(X_test)[:,1] > THRESHOLD, 1, 0)'''\n",
    "    yprobs = model.predict_log_proba(X_test)\n",
    "    yprobs = yprobs[:,1]\n",
    "    ras = roc_auc_score(y_test,yprobs,average = 'weighted')\n",
    "    print(ras)\n",
    "    yhat = model.predict(X_test)\n",
    "    \n",
    "    #pd.DataFrame(yhat).to_csv(model)\n",
    "    TP, TN, FP, FN = 0,0,0,0\n",
    "    for i in range(len(yhat)):\n",
    "        if yhat[i] == 0:\n",
    "            if y_test[i] == 0:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            if y_test[i] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    print(classification_report(y_test, yhat))\n",
    "    print(classification_report(y_test,yhat,output_dict = True)['1']['precision'], classification_report(y_test, yhat, output_dict = True)['1']['recall'])\n",
    "    fpr, tpr, threshold = roc_curve(y_test, yprobs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    return([TP, TN, FP, FN, TP/(TP + FN), TN/(TN + FP)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Scores for SVC\n",
    "def getScoresForSVC(model, X_train, X_test, y_train, y_test):\n",
    "    yhat = model.predict(X_test)  \n",
    "    #pd.DataFrame(yhat).to_csv(model)\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    for i in range(len(yhat)):\n",
    "        if yhat[i] == 0:\n",
    "            if y_test[i] == 0:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            if y_test[i] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    print(classification_report(y_test, yhat))\n",
    "    print(classification_report(y_test, yhat, output_dict=True)['1']['precision'], classification_report(y_test, yhat, output_dict = True)['1']['recall'])\n",
    "    return([TP, TN, FP, FN, TP/(TP + FN), TN/(TN + FP)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.cumsum(pca.explained_variance_ratio_) [0.10556106 0.15411495 0.19643988 0.23476932 0.2719896  0.3049957\n",
      " 0.3359417  0.36529616 0.39313832 0.42013794 0.44601909 0.47087749\n",
      " 0.49432268 0.51660722 0.53833372 0.55899851 0.57945657 0.59973984\n",
      " 0.61987363 0.6398666  0.6593861  0.67761315 0.69576557 0.71344004\n",
      " 0.73105196 0.74848247 0.76573186 0.78243455 0.79904326 0.81549941\n",
      " 0.83176839 0.84745606 0.86288147 0.87829062 0.89315374 0.9074046\n",
      " 0.92053081 0.93347961 0.94606333 0.95672836 0.96512835 0.97160422\n",
      " 0.97790947 0.98211859 0.986221   0.98967053 0.99294856 0.99488901\n",
      " 0.99666183 0.99788993 0.99891246 0.99989747 0.99996474 0.99999991\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-e26fc2aa5ad4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtrainx_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestx_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaleFetures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainx_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestx_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'Standard'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtrainy_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencodeLabelsToZeroAndOne\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainy_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrainx_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestx_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindPrincipalComponents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainx_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestx_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplitTrainAndTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainx_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainy_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-25de5bad759c>\u001b[0m in \u001b[0;36mfindPrincipalComponents\u001b[1;34m(trainx_df, testx_df)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Number of Components'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Variance (%)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#for each component\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tune regularization Parameter based on f1 value or mse\n",
    "def predictTestx(Model, testx_df):\n",
    "    testpred = pd.DataFrame(Model.predict(testx_df))\n",
    "    testpred.to_csv(\"test_pred.csv\")\n",
    "\n",
    "trainx_df, trainy_df, testx_df = readDataSets(\"C:\\\\Users\\\\M. LAKSHMI MADHURI\\\\Desktop\\\\Repositories\\\\Data_Science_2019501105\\\\Intro to ML\\\\CodeCamp2\\\\marketing_training.csv\",\"C:\\\\Users\\\\M. LAKSHMI MADHURI\\\\Desktop\\\\Repositories\\\\Data_Science_2019501105\\\\Intro to ML\\\\CodeCamp2\\\\marketing_test.csv\", predict_col='responded')\n",
    "trainx_df, testx_df = dropFeturesWithNullValuesGreaterThanALimit(trainx_df, testx_df,null_ratio=0.5)\n",
    "trainx_df, testx_df = oneHotEncode(trainx_df, testx_df)\n",
    "trainx_df, testx_df = fillMissingValues(trainx_df, testx_df)\n",
    "trainx_df, testx_df = scaleFetures(trainx_df, testx_df, scale ='Standard')\n",
    "trainy_df = encodeLabelsToZeroAndOne(trainy_df)\n",
    "trainx_df,testx_df = findPrincipalComponents(trainx_df, testx_df)\n",
    "X_train, X_test, y_train, y_test = splitTrainAndTest(trainx_df, trainy_df, split_ratio = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-1ea40dfc734f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mLogRegModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetLogisticRegressionModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgetScores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogRegModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Results for SVM Classifier\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-3dece66c07d7>\u001b[0m in \u001b[0;36mgetScores\u001b[1;34m(model, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0myprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_log_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0myprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myprobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mras\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    379\u001b[0m                              \"instead\".format(max_fpr))\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         return _multiclass_roc_auc_score(y_true, y_score, labels,\n\u001b[0;32m    383\u001b[0m                                          multi_class, average, sample_weight)\n",
      "\u001b[1;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "LogRegModel = getLogisticRegressionModel(X_train, y_train)\n",
    "getScores(LogRegModel, X_train, X_test, y_train, y_test)\n",
    "print(\"Results for SVM Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       35311       0.00      0.00      0.00         1\n",
      "       37900       0.00      0.00      0.00         0\n",
      "       40000       0.00      0.00      0.00         1\n",
      "       55993       0.00      0.00      0.00         1\n",
      "       60000       0.00      0.00      0.00         1\n",
      "       62383       0.00      0.00      0.00         1\n",
      "       64500       0.00      0.00      0.00         1\n",
      "       66500       0.00      0.00      0.00         1\n",
      "       67000       0.00      0.00      0.00         2\n",
      "       68400       0.00      0.00      0.00         1\n",
      "       68500       0.00      0.00      0.00         1\n",
      "       75000       0.00      0.00      0.00         1\n",
      "       75500       0.00      0.00      0.00         1\n",
      "       79000       0.00      0.00      0.00         1\n",
      "       79500       0.00      0.00      0.00         1\n",
      "       81000       0.00      0.00      0.00         1\n",
      "       82500       0.00      0.00      0.00         1\n",
      "       84500       0.00      0.00      0.00         1\n",
      "       84900       0.00      0.00      0.00         1\n",
      "       85000       0.00      0.00      0.00         2\n",
      "       85400       0.00      0.00      0.00         1\n",
      "       86000       0.00      0.00      0.00         2\n",
      "       87500       0.00      0.00      0.00         1\n",
      "       89471       0.00      0.00      0.00         1\n",
      "       90000       0.00      0.00      0.00         0\n",
      "       91000       0.00      0.00      0.00         1\n",
      "       91300       0.00      0.00      0.00         1\n",
      "       92000       0.00      0.00      0.00         1\n",
      "       92900       0.00      0.00      0.00         0\n",
      "       93500       0.00      0.00      0.00         2\n",
      "       94000       0.00      0.00      0.00         1\n",
      "       96500       0.00      0.00      0.00         1\n",
      "       97000       0.00      0.00      0.00         1\n",
      "       98000       0.00      0.00      0.00         1\n",
      "      100000       0.00      0.00      0.00         1\n",
      "      101800       0.00      0.00      0.00         1\n",
      "      102000       0.00      0.00      0.00         1\n",
      "      102776       0.00      0.00      0.00         1\n",
      "      103200       0.00      0.00      0.00         1\n",
      "      104000       0.00      0.00      0.00         1\n",
      "      105000       0.00      0.00      0.00         1\n",
      "      105900       0.00      0.00      0.00         1\n",
      "      106000       0.00      0.00      0.00         1\n",
      "      107000       0.00      0.00      0.00         1\n",
      "      107500       0.00      0.00      0.00         2\n",
      "      107900       0.00      0.00      0.00         1\n",
      "      108000       0.00      0.00      0.00         1\n",
      "      108480       0.00      0.00      0.00         1\n",
      "      108959       0.00      0.00      0.00         1\n",
      "      109000       0.00      0.00      0.00         1\n",
      "      109008       0.00      0.00      0.00         1\n",
      "      109500       0.00      0.00      0.00         1\n",
      "      110000       0.00      0.00      0.00         2\n",
      "      112000       0.00      0.00      0.00         1\n",
      "      113000       0.00      0.00      0.00         3\n",
      "      114500       0.00      0.00      0.00         1\n",
      "      115000       0.00      0.00      0.00         8\n",
      "      116050       0.00      0.00      0.00         1\n",
      "      117500       0.00      0.00      0.00         1\n",
      "      118000       0.00      0.00      0.00         1\n",
      "      118400       0.00      0.00      0.00         1\n",
      "      118500       0.00      0.00      0.00         1\n",
      "      118858       0.00      0.00      0.00         1\n",
      "      119000       0.00      0.00      0.00         2\n",
      "      119500       0.00      0.00      0.00         2\n",
      "      119900       0.00      0.00      0.00         1\n",
      "      120000       0.00      0.00      0.00         3\n",
      "      121000       0.00      0.00      0.00         0\n",
      "      122500       0.00      0.00      0.00         1\n",
      "      123000       0.00      0.00      0.00         2\n",
      "      124000       0.00      0.00      0.00         2\n",
      "      124500       0.00      0.00      0.00         2\n",
      "      124900       0.00      0.00      0.00         1\n",
      "      125000       0.00      0.00      0.00         1\n",
      "      125500       0.00      0.00      0.00         2\n",
      "      126000       0.00      0.00      0.00         1\n",
      "      126175       0.00      0.00      0.00         1\n",
      "      126500       0.00      0.00      0.00         1\n",
      "      127000       0.00      0.00      0.00         4\n",
      "      127500       0.00      0.00      0.00         2\n",
      "      128000       0.00      0.00      0.00         1\n",
      "      128200       0.00      0.00      0.00         1\n",
      "      128500       0.00      0.00      0.00         3\n",
      "      128950       0.00      0.00      0.00         1\n",
      "      129000       0.00      0.00      0.00         3\n",
      "      129500       0.00      0.00      0.00         1\n",
      "      129900       0.00      0.00      0.00         1\n",
      "      130000       0.00      0.00      0.00         4\n",
      "      130250       0.00      0.00      0.00         1\n",
      "      130500       0.00      0.00      0.00         2\n",
      "      131000       0.00      0.00      0.00         1\n",
      "      131400       0.00      0.00      0.00         1\n",
      "      132000       0.00      0.00      0.00         2\n",
      "      132500       0.00      0.00      0.00         1\n",
      "      133000       0.00      0.00      0.00         3\n",
      "      133900       0.00      0.00      0.00         1\n",
      "      134000       0.00      0.00      0.00         2\n",
      "      134432       0.00      0.00      0.00         1\n",
      "      134500       0.00      0.00      0.00         2\n",
      "      134900       0.00      0.00      0.00         1\n",
      "      135000       0.00      0.00      0.00         4\n",
      "      135500       0.00      0.00      0.00         2\n",
      "      135750       0.00      0.00      0.00         1\n",
      "      135900       0.00      0.00      0.00         1\n",
      "      136000       0.00      0.00      0.00         1\n",
      "      136500       0.00      0.00      0.00         2\n",
      "      137000       0.00      0.00      0.00         1\n",
      "      137500       0.00      0.00      0.00         3\n",
      "      138000       0.00      0.00      0.00         1\n",
      "      139000       0.00      0.00      0.00         3\n",
      "      139900       0.00      0.00      0.00         1\n",
      "      140000       0.01      0.83      0.02         6\n",
      "      141000       0.00      0.00      0.00         3\n",
      "      142500       0.00      0.00      0.00         1\n",
      "      142953       0.00      0.00      0.00         0\n",
      "      143000       0.00      0.00      0.00         4\n",
      "      143500       0.00      0.00      0.00         1\n",
      "      144000       0.00      0.00      0.00         4\n",
      "      144500       0.00      0.00      0.00         2\n",
      "      145000       0.00      0.00      0.00         7\n",
      "      146000       0.00      0.00      0.00         2\n",
      "      147000       0.00      0.00      0.00         3\n",
      "      148500       0.00      0.00      0.00         1\n",
      "      149000       0.00      0.00      0.00         2\n",
      "      149350       0.00      0.00      0.00         1\n",
      "      149500       0.00      0.00      0.00         2\n",
      "      149900       0.00      0.00      0.00         1\n",
      "      150000       0.00      0.00      0.00         2\n",
      "      151000       0.00      0.00      0.00         1\n",
      "      151500       0.00      0.00      0.00         1\n",
      "      152000       0.00      0.00      0.00         2\n",
      "      153000       0.00      0.00      0.00         1\n",
      "      153337       0.00      0.00      0.00         1\n",
      "      153500       0.00      0.00      0.00         2\n",
      "      153575       0.00      0.00      0.00         1\n",
      "      153900       0.00      0.00      0.00         1\n",
      "      154000       0.00      0.00      0.00         1\n",
      "      154300       0.00      0.00      0.00         1\n",
      "      154500       0.00      0.00      0.00         1\n",
      "      154900       0.00      0.00      0.00         1\n",
      "      155000       0.00      0.00      0.00         5\n",
      "      155835       0.00      0.00      0.00         1\n",
      "      155900       0.00      0.00      0.00         1\n",
      "      156000       0.00      0.00      0.00         1\n",
      "      156500       0.00      0.00      0.00         1\n",
      "      157000       0.00      0.00      0.00         1\n",
      "      157900       0.00      0.00      0.00         1\n",
      "      158000       0.00      0.00      0.00         1\n",
      "      159000       0.00      0.00      0.00         2\n",
      "      159500       0.00      0.00      0.00         1\n",
      "      159950       0.00      0.00      0.00         1\n",
      "      160000       0.00      0.00      0.00         3\n",
      "      161500       0.00      0.00      0.00         1\n",
      "      162900       0.00      0.00      0.00         1\n",
      "      163500       0.00      0.00      0.00         1\n",
      "      164700       0.00      0.00      0.00         1\n",
      "      165000       0.00      0.00      0.00         3\n",
      "      165600       0.00      0.00      0.00         1\n",
      "      167500       0.00      0.00      0.00         0\n",
      "      168000       0.00      0.00      0.00         1\n",
      "      169000       0.00      0.00      0.00         1\n",
      "      169990       0.00      0.00      0.00         1\n",
      "      170000       0.00      0.00      0.00         1\n",
      "      171000       0.00      0.00      0.00         2\n",
      "      172400       0.00      0.00      0.00         1\n",
      "      172500       0.00      0.00      0.00         2\n",
      "      173000       0.00      0.00      0.00         2\n",
      "      173500       0.00      0.00      0.00         1\n",
      "      174000       0.00      0.00      0.00         2\n",
      "      175000       0.00      0.00      0.00         5\n",
      "      175500       0.00      0.00      0.00         1\n",
      "      175900       0.00      0.00      0.00         1\n",
      "      176000       0.00      0.00      0.00         3\n",
      "      177500       0.00      0.00      0.00         2\n",
      "      178000       0.00      0.00      0.00         3\n",
      "      178900       0.00      0.00      0.00         1\n",
      "      179900       0.00      0.00      0.00         2\n",
      "      180000       0.00      0.00      0.00         3\n",
      "      180500       0.00      0.00      0.00         1\n",
      "      181000       0.00      0.00      0.00         3\n",
      "      181134       0.00      0.00      0.00         1\n",
      "      182000       0.00      0.00      0.00         1\n",
      "      183500       0.00      0.00      0.00         1\n",
      "      183900       0.00      0.00      0.00         1\n",
      "      184000       0.00      0.00      0.00         1\n",
      "      184900       0.00      0.00      0.00         1\n",
      "      185000       0.00      0.00      0.00         4\n",
      "      185750       0.00      0.00      0.00         1\n",
      "      185900       0.00      0.00      0.00         1\n",
      "      187000       0.00      0.00      0.00         0\n",
      "      187100       0.00      0.00      0.00         1\n",
      "      187500       0.00      0.00      0.00         1\n",
      "      187750       0.00      0.00      0.00         1\n",
      "      188000       0.00      0.00      0.00         1\n",
      "      189000       0.00      0.00      0.00         2\n",
      "      190000       0.00      0.00      0.00         3\n",
      "      191000       0.00      0.00      0.00         1\n",
      "      192000       0.00      0.00      0.00         1\n",
      "      192140       0.00      0.00      0.00         1\n",
      "      192500       0.00      0.00      0.00         1\n",
      "      194000       0.00      0.00      0.00         1\n",
      "      194201       0.00      0.00      0.00         1\n",
      "      194500       0.00      0.00      0.00         1\n",
      "      195000       0.00      0.00      0.00         1\n",
      "      195400       0.00      0.00      0.00         1\n",
      "      196000       0.00      0.00      0.00         2\n",
      "      197000       0.00      0.00      0.00         2\n",
      "      197900       0.00      0.00      0.00         1\n",
      "      200000       0.00      0.00      0.00         2\n",
      "      200500       0.00      0.00      0.00         1\n",
      "      200624       0.00      0.00      0.00         1\n",
      "      201000       0.00      0.00      0.00         1\n",
      "      202500       0.00      0.00      0.00         1\n",
      "      204900       0.00      0.00      0.00         1\n",
      "      205000       0.00      0.00      0.00         4\n",
      "      205950       0.00      0.00      0.00         1\n",
      "      206300       0.00      0.00      0.00         1\n",
      "      207000       0.00      0.00      0.00         1\n",
      "      207500       0.00      0.00      0.00         2\n",
      "      208900       0.00      0.00      0.00         1\n",
      "      210000       0.00      0.00      0.00         1\n",
      "      213000       0.00      0.00      0.00         2\n",
      "      213500       0.00      0.00      0.00         1\n",
      "      214000       0.00      0.00      0.00         2\n",
      "      215000       0.00      0.00      0.00         1\n",
      "      216000       0.00      0.00      0.00         1\n",
      "      217000       0.00      0.00      0.00         1\n",
      "      217500       0.00      0.00      0.00         1\n",
      "      219210       0.00      0.00      0.00         1\n",
      "      219500       0.00      0.00      0.00         1\n",
      "      220000       0.00      0.00      0.00         1\n",
      "      222500       0.00      0.00      0.00         1\n",
      "      224000       0.00      0.00      0.00         1\n",
      "      224900       0.00      0.00      0.00         1\n",
      "      225000       0.00      0.00      0.00         3\n",
      "      226000       0.00      0.00      0.00         1\n",
      "      228000       0.00      0.00      0.00         0\n",
      "      228500       0.00      0.00      0.00         1\n",
      "      230000       0.00      0.00      0.00         3\n",
      "      233170       0.00      0.00      0.00         1\n",
      "      236000       0.00      0.00      0.00         1\n",
      "      238000       0.00      0.00      0.00         1\n",
      "      239000       0.00      0.00      0.00         2\n",
      "      239500       0.00      0.00      0.00         1\n",
      "      240000       0.00      0.00      0.00         2\n",
      "      241500       0.00      0.00      0.00         1\n",
      "      242000       0.00      0.00      0.00         1\n",
      "      243000       0.00      0.00      0.00         1\n",
      "      244000       0.00      0.00      0.00         1\n",
      "      244400       0.00      0.00      0.00         1\n",
      "      245350       0.00      0.00      0.00         1\n",
      "      246578       0.00      0.00      0.00         1\n",
      "      248000       0.00      0.00      0.00         1\n",
      "      250000       0.00      0.00      0.00         2\n",
      "      253000       0.00      0.00      0.00         1\n",
      "      253293       0.00      0.00      0.00         1\n",
      "      254000       0.00      0.00      0.00         1\n",
      "      255000       0.00      0.00      0.00         2\n",
      "      260000       0.00      0.00      0.00         3\n",
      "      262000       0.00      0.00      0.00         1\n",
      "      262500       0.00      0.00      0.00         1\n",
      "      263000       0.00      0.00      0.00         1\n",
      "      264132       0.00      0.00      0.00         1\n",
      "      266000       0.00      0.00      0.00         1\n",
      "      270000       0.00      0.00      0.00         1\n",
      "      271000       0.00      0.00      0.00         1\n",
      "      274900       0.00      0.00      0.00         1\n",
      "      275000       0.00      0.00      0.00         3\n",
      "      276000       0.00      0.00      0.00         1\n",
      "      277000       0.00      0.00      0.00         1\n",
      "      280000       0.00      0.00      0.00         1\n",
      "      283463       0.00      0.00      0.00         1\n",
      "      284000       0.00      0.00      0.00         2\n",
      "      285000       0.00      0.00      0.00         1\n",
      "      287000       0.00      0.00      0.00         1\n",
      "      293077       0.00      0.00      0.00         1\n",
      "      297000       0.00      0.00      0.00         1\n",
      "      301000       0.00      0.00      0.00         1\n",
      "      305000       0.00      0.00      0.00         1\n",
      "      310000       0.00      0.00      0.00         2\n",
      "      311500       0.00      0.00      0.00         1\n",
      "      311872       0.00      0.00      0.00         1\n",
      "      314813       0.00      0.00      0.00         1\n",
      "      315000       0.00      0.00      0.00         1\n",
      "      315500       0.00      0.00      0.00         1\n",
      "      317000       0.00      0.00      0.00         1\n",
      "      318061       0.00      0.00      0.00         1\n",
      "      325000       0.00      0.00      0.00         2\n",
      "      335000       0.00      0.00      0.00         2\n",
      "      337500       0.00      0.00      0.00         1\n",
      "      340000       0.00      0.00      0.00         1\n",
      "      341000       0.00      0.00      0.00         1\n",
      "      348000       0.00      0.00      0.00         1\n",
      "      350000       0.00      0.00      0.00         1\n",
      "      360000       0.00      0.00      0.00         1\n",
      "      367294       0.00      0.00      0.00         1\n",
      "      369900       0.00      0.00      0.00         1\n",
      "      374000       0.00      0.00      0.00         1\n",
      "      380000       0.00      0.00      0.00         1\n",
      "      395000       0.00      0.00      0.00         1\n",
      "      402000       0.00      0.00      0.00         1\n",
      "      403000       0.00      0.00      0.00         1\n",
      "      423000       0.00      0.00      0.00         1\n",
      "      438780       0.00      0.00      0.00         1\n",
      "      446261       0.00      0.00      0.00         1\n",
      "      451950       0.00      0.00      0.00         1\n",
      "      465000       0.00      0.00      0.00         1\n",
      "      485000       0.00      0.00      0.00         1\n",
      "      556581       0.00      0.00      0.00         1\n",
      "      611657       0.00      0.00      0.00         1\n",
      "      755000       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.01       438\n",
      "   macro avg       0.00      0.00      0.00       438\n",
      "weighted avg       0.00      0.01      0.00       438\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M. LAKSHMI MADHURI\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\M. LAKSHMI MADHURI\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-5f98e5f0470c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msvcmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetSVClassificationModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_par\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'poly'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgetScoresForSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvcmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Results for Back Propagation Classifier\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnn_bp_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetBackPropagationModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_par\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhid_layer_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxi_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgetScores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_bp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-14e7c0e9e1f7>\u001b[0m in \u001b[0;36mgetScoresForSVC\u001b[1;34m(model, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mFP\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'precision'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'recall'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTP\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTN\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTN\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '1'"
     ]
    }
   ],
   "source": [
    "svcmodel = getSVClassificationModel(X_train, y_train, reg_par = 0.5, deg = 2, ker = 'poly')\n",
    "getScoresForSVC(svcmodel, X_train, X_test, y_train, y_test)\n",
    "print(\"Results for Back Propagation Classifier\")\n",
    "nn_bp_model = getBackPropagationModel(X_train, y_train, sol = 'lbfgs', reg_par = 0.01, hid_layer_sizes = (7, ), random_state = 1, maxi_iter = 10000)\n",
    "getScores(nn_bp_model, X_train, X_test, y_train, y_test)\n",
    "'''LRModel=getLinearRegressionModel(X_train, y_train)\n",
    "RidgeModel=getRidgeRegressionModel(X_train, y_train,tolerence=0.0001,reg_par=0.5)\n",
    "getRSqureandMSEVsAlphaPlots(X_train, X_test, y_train, y_test,alpha_start=130,alpha_end=146,jumps=10)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
